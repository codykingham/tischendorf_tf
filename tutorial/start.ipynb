{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"images/tf.png\" width=\"128\"/>\n",
    "<img align=\"right\" src=\"images/logo.png\"/>\n",
    "\n",
    "# Tutorial\n",
    "\n",
    "This notebook gets you started with using\n",
    "[Text-Fabric](https://annotation.github.io/text-fabric/) for coding in the Tischendorf New Testament.\n",
    "\n",
    "Familiarity with the underlying\n",
    "[data model](https://annotation.github.io/text-fabric/tf/about/datamodel.html)\n",
    "is recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Text-Fabric\n",
    "\n",
    "### Python\n",
    "\n",
    "You need to have Python on your system. Most systems have it out of the box,\n",
    "but alas, that is python2 and we need at least python **3.6**.\n",
    "\n",
    "Install it from [python.org](https://www.python.org) or from\n",
    "[Anaconda](https://www.anaconda.com/download).\n",
    "\n",
    "### TF itself\n",
    "\n",
    "```\n",
    "pip3 install text-fabric\n",
    "```\n",
    "\n",
    "### Jupyter notebook\n",
    "\n",
    "You need [Jupyter](http://jupyter.org).\n",
    "\n",
    "If it is not already installed:\n",
    "\n",
    "```\n",
    "pip3 install jupyter\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T10:40:34.922214Z",
     "start_time": "2018-10-18T10:40:34.901689Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T10:40:36.468086Z",
     "start_time": "2018-10-18T10:40:36.442023Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T10:40:37.964346Z",
     "start_time": "2018-10-18T10:40:37.346091Z"
    }
   },
   "outputs": [],
   "source": [
    "from tf.app import use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tischendorf data\n",
    "\n",
    "Text-Fabric will fetch a standard set of features for you from the newest github release binaries.\n",
    "\n",
    "The data will be stored in the `text-fabric-data` in your home directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Features\n",
    "The data of the corpus is organized in features.\n",
    "They are *columns* of data.\n",
    "Think of the text as a gigantic spreadsheet, where row 1 corresponds to the\n",
    "first word, row 2 to the second word, and so on, for all 100,000+ words.\n",
    "\n",
    "The letters of each word is a column `form` in that spreadsheet.\n",
    "\n",
    "The corpus contains ca. 30 columns, not only for the words, but also for\n",
    "textual objects, such as *books*, *chapters*, and *verses*.\n",
    "\n",
    "Instead of putting that information in one big table, the data is organized in separate columns.\n",
    "We call those columns **features**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the very last version, use `hot`.\n",
    "\n",
    "For the latest release, use `latest`.\n",
    "\n",
    "If you have cloned the repos (TF app and data), use `clone`.\n",
    "\n",
    "If you do not want/need to upgrade, leave out the checkout specifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T10:40:42.958397Z",
     "start_time": "2018-10-18T10:40:41.535490Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b title=\"local github\">TF-app:</b> <span title=\"repo clone offline under ~/github\">~/github/annotation/app-tisch/code</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local github\">data:</b> <span title=\"repo clone offline under ~/github\">~/github/codykingham/tischendorf_tf/tf/2.8</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |      |     0.01s C __levels__           from otype, oslots, otext\n",
      "   |      |     1.00s C __order__            from otype, oslots, __levels__\n",
      "   |      |     0.05s C __rank__             from otype, __order__\n",
      "   |      |     1.31s C __levUp__            from otype, oslots, __rank__\n",
      "   |      |     0.05s C __levDown__          from otype, __levUp__, __rank__\n",
      "   |      |     0.49s C __boundary__         from otype, oslots, __rank__\n",
      "   |      |     0.03s C __sections__         from otype, oslots, otext, __levUp__, __levels__, book, chapter, verse\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Text-Fabric:</b> <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/cheatsheet.html\" title=\"text-fabric-api\">Text-Fabric API 8.3.0</a>, <a target=\"_blank\" href=\"https://github.com/annotation/app-tisch\" title=\"tisch TF-app\">app-tisch</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/about/searchusage.html\" title=\"Search Templates Introduction and Reference\">Search Reference</a><br><b>Data:</b> <a target=\"_blank\" href=\"https://github.com/codykingham/tischendorf_tf/blob/master/docs/features.md\" title=\"provenance of Tischendorf's 8th New Testament\">TISCHENDORF_TF</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/writing/greek.html\" title=\"How TF features represent text\">Character table</a>, <a target=\"_blank\" href=\"https://github.com/codykingham/tischendorf_tf/blob/master/docs/features.md#features\" title=\"TISCHENDORF_TF feature documentation\">Feature docs</a><br><b>Features:</b><br><details><summary><b>Tischendorf's 8th New Testament</b></summary><a target=\"_blank\" href=\"https://github.com/codykingham/tischendorf_tf/blob/master/docs/features.md#anlex_lem\" title=\"~/github/codykingham/tischendorf_tf/tf/2.8/anlex_lem.tf\">anlex_lem</a><br><a target=\"_blank\" href=\"https://github.com/codykingham/tischendorf_tf/blob/master/docs/features.md#book\" title=\"~/github/codykingham/tischendorf_tf/tf/2.8/book.tf\">book</a><br><a target=\"_blank\" href=\"https://github.com/codykingham/tischendorf_tf/blob/master/docs/features.md#book_code\" title=\"~/github/codykingham/tischendorf_tf/tf/2.8/book_code.tf\">book_code</a><br><a target=\"_blank\" href=\"https://github.com/codykingham/tischendorf_tf/blob/master/docs/features.md#chapter\" title=\"~/github/codykingham/tischendorf_tf/tf/2.8/chapter.tf\">chapter</a><br><a target=\"_blank\" href=\"https://github.com/codykingham/tischendorf_tf/blob/master/docs/features.md#freq_lex\" title=\"~/github/codykingham/tischendorf_tf/tf/2.8/freq_lex.tf\">freq_lex</a><br><a target=\"_blank\" href=\"https://github.com/codykingham/tischendorf_tf/blob/master/docs/features.md#gloss\" title=\"~/github/codykingham/tischendorf_tf/tf/2.8/gloss.tf\">gloss</a><br><a target=\"_blank\" href=\"https://github.com/codykingham/tischendorf_tf/blob/master/docs/features.md#ketiv\" title=\"~/github/codykingham/tischendorf_tf/tf/2.8/ketiv.tf\">ketiv</a><br><a target=\"_blank\" href=\"https://github.com/codykingham/tischendorf_tf/blob/master/docs/features.md#morph\" title=\"~/github/codykingham/tischendorf_tf/tf/2.8/morph.tf\">morph</a><br><a target=\"_blank\" href=\"https://github.com/codykingham/tischendorf_tf/blob/master/docs/features.md#otype\" title=\"~/github/codykingham/tischendorf_tf/tf/2.8/otype.tf\">otype</a><br><a target=\"_blank\" href=\"https://github.com/codykingham/tischendorf_tf/blob/master/docs/features.md#para\" title=\"~/github/codykingham/tischendorf_tf/tf/2.8/para.tf\">para</a><br><a target=\"_blank\" href=\"https://github.com/codykingham/tischendorf_tf/blob/master/docs/features.md#qere\" title=\"~/github/codykingham/tischendorf_tf/tf/2.8/qere.tf\">qere</a><br><a target=\"_blank\" href=\"https://github.com/codykingham/tischendorf_tf/blob/master/docs/features.md#str_lem\" title=\"~/github/codykingham/tischendorf_tf/tf/2.8/str_lem.tf\">str_lem</a><br><a target=\"_blank\" href=\"https://github.com/codykingham/tischendorf_tf/blob/master/docs/features.md#strongs\" title=\"~/github/codykingham/tischendorf_tf/tf/2.8/strongs.tf\">strongs</a><br><a target=\"_blank\" href=\"https://github.com/codykingham/tischendorf_tf/blob/master/docs/features.md#verse\" title=\"~/github/codykingham/tischendorf_tf/tf/2.8/verse.tf\">verse</a><br><a target=\"_blank\" href=\"https://github.com/codykingham/tischendorf_tf/blob/master/docs/features.md#vrsnum\" title=\"~/github/codykingham/tischendorf_tf/tf/2.8/vrsnum.tf\">vrsnum</a><br><b><i><a target=\"_blank\" href=\"https://github.com/codykingham/tischendorf_tf/blob/master/docs/features.md#oslots\" title=\"~/github/codykingham/tischendorf_tf/tf/2.8/oslots.tf\">oslots</a></i></b><br></details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>tr.tf.ltr, td.tf.ltr, th.tf.ltr { text-align: left ! important;}\n",
       "tr.tf.rtl, td.tf.rtl, th.tf.rtl { text-align: right ! important;}\n",
       "@font-face {\n",
       "  font-family: \"Gentium Plus\";\n",
       "  src: local('Gentium Plus'), local('GentiumPlus'),\n",
       "    url('/server/static/fonts/GentiumPlus-R.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/GentiumPlus-R.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Ezra SIL\";\n",
       "  src: local('Ezra SIL'), local('EzraSIL'),\n",
       "    url('/server/static/fonts/SILEOT.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SILEOT.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"SBL Hebrew\";\n",
       "  src: local('SBL Hebrew'), local('SBLHebrew'),\n",
       "    url('/server/static/fonts/SBL_Hbrw.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SBL_Hbrw.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Estrangelo Edessa\";\n",
       "  src: local('Estrangelo Edessa'), local('EstrangeloEdessa');\n",
       "    url('/server/static/fonts/SyrCOMEdessa.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SyrCOMEdessa.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: AmiriQuran;\n",
       "  font-style: normal;\n",
       "  font-weight: 400;\n",
       "  src: local('Amiri Quran'), local('AmiriQuran'),\n",
       "    url('/server/static/fonts/AmiriQuran.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuran.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: AmiriQuranColored;\n",
       "  font-style: normal;\n",
       "  font-weight: 400;\n",
       "  src: local('Amiri Quran Colored'), local('AmiriQuranColored'),\n",
       "    url('/server/static/fonts/AmiriQuranColored.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuranColored.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Santakku\";\n",
       "  src: local('Santakku'),\n",
       "    url('/server/static/fonts/Santakku.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/Santakku.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"SantakkuM\";\n",
       "  src: local('SantakkuM'),\n",
       "    url('/server/static/fonts/SantakkuM.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SantakkuM.woff?raw=true') format('woff');\n",
       "}\n",
       "/* bypassing some classical notebook settings */\n",
       "div#notebook {\n",
       "  line-height: unset;\n",
       "}\n",
       "/* neutral text */\n",
       ".txtn,.txtn a:visited,.txtn a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* transcription text */\n",
       ".txtt,.txtt a:visited,.txtt a:link {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* source text */\n",
       ".txto,.txto a:visited,.txto a:link {\n",
       "    font-family: serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* phonetic text */\n",
       ".txtp,.txtp a:visited,.txtp a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* original script text */\n",
       ".txtu,.txtu a:visited,.txtu a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* hebrew */\n",
       ".txtu.hbo,.lex.hbo {\n",
       "    font-family: \"Ezra SIL\", \"SBL Hebrew\", sans-serif;\n",
       "    font-size: large;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* syriac */\n",
       ".txtu.syc,.lex.syc {\n",
       "    font-family: \"Estrangelo Edessa\", sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* neo aramaic */\n",
       ".txtu.cld,.lex.cld {\n",
       "    font-family: \"CharisSIL-R\", sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* standard arabic */\n",
       ".txtu.ara,.lex.ara {\n",
       "    font-family: \"AmiriQuran\", sans-serif;\n",
       "    font-size: large;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* cuneiform */\n",
       ".txtu.akk,.lex.akk {\n",
       "    font-family: Santakku, sans-serif;\n",
       "    font-size: large;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* greek */\n",
       ".txtu.grc,.lex.grc a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "a:hover {\n",
       "    text-decoration: underline | important;\n",
       "    color: #0000ff | important;\n",
       "}\n",
       ".ltr {\n",
       "    direction: ltr ! important;\n",
       "}\n",
       ".rtl {\n",
       "    direction: rtl ! important;\n",
       "}\n",
       ".features {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: var(--features);\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    padding: 0.1rem;\n",
       "    margin: 0.1rem;\n",
       "    direction: ltr;\n",
       "    border: var(--meta-width) solid var(--meta-color);\n",
       "    border-radius: var(--meta-width);\n",
       "}\n",
       ".features div,.features span {\n",
       "    padding: 0;\n",
       "    margin: -0.1rem 0;\n",
       "}\n",
       ".features .f {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: normal;\n",
       "    color: #5555bb;\n",
       "}\n",
       ".features .xft {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: medium;\n",
       "  margin: 0.1rem 0rem;\n",
       "}\n",
       ".features .xft .f {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: small;\n",
       "  font-weight: normal;\n",
       "}\n",
       ".section {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: var(--section);\n",
       "    unicode-bidi: embed;\n",
       "    text-align: start;\n",
       "}\n",
       ".structure {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: var(--structure);\n",
       "    unicode-bidi: embed;\n",
       "    text-align: start;\n",
       "}\n",
       ".comments {\n",
       "    display: flex;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    flex-flow: column nowrap;\n",
       "}\n",
       ".nd, a:link.nd {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    color: var(--node);\n",
       "    vertical-align: super;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".lex {\n",
       "  color: var(--lex-color);;\n",
       "}\n",
       ".children,.children.ltr {\n",
       "    display: flex;\n",
       "    border: 0;\n",
       "    background-color: #ffffff;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "}\n",
       ".children.stretch {\n",
       "    align-items: stretch;\n",
       "}\n",
       ".children.hor {\n",
       "    flex-flow: row nowrap;\n",
       "}\n",
       ".children.hor.wrap {\n",
       "    flex-flow: row wrap;\n",
       "}\n",
       ".children.ver {\n",
       "    flex-flow: column nowrap;\n",
       "}\n",
       ".children.ver.wrap {\n",
       "    flex-flow: column wrap;\n",
       "}\n",
       ".contnr {\n",
       "    width: fit-content;\n",
       "    display: flex;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    flex-flow: column nowrap;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding:  0.5rem 0.1rem 0.1rem 0.1rem;\n",
       "    margin: 0.8rem 0.1rem 0.1rem 0.1rem;\n",
       "    border-style: solid;\n",
       "    font-size: small;\n",
       "}\n",
       ".contnr.trm {\n",
       "    background-attachment: local;\n",
       "}\n",
       ".contnr.cnul {\n",
       "    padding:  0;\n",
       "    margin: 0;\n",
       "    border-style: solid;\n",
       "    font-size: xx-small;\n",
       "}\n",
       ".contnr.cnul,.lbl.cnul {\n",
       "    border-color: var(--border-color-nul);\n",
       "    border-width: var(--border-width-nul);\n",
       "    border-radius: var(--border-width-nul);\n",
       "}\n",
       ".contnr.c0,.lbl.c0 {\n",
       "    border-color: var(--border-color0);\n",
       "    border-width: var(--border-width0);\n",
       "    border-radius: var(--border-width0);\n",
       "}\n",
       ".contnr.c1,.lbl.c1 {\n",
       "    border-color: var(--border-color1);\n",
       "    border-width: var(--border-width1);\n",
       "    border-radius: var(--border-width1);\n",
       "}\n",
       ".contnr.c2,.lbl.c2 {\n",
       "    border-color: var(--border-color2);\n",
       "    border-width: var(--border-width2);\n",
       "    border-radius: var(--border-width2);\n",
       "}\n",
       ".contnr.c3,.lbl.c3 {\n",
       "    border-color: var(--border-color3);\n",
       "    border-width: var(--border-width3);\n",
       "    border-radius: var(--border-width3);\n",
       "}\n",
       ".contnr.c4,.lbl.c4 {\n",
       "    border-color: var(--border-color4);\n",
       "    border-width: var(--border-width4);\n",
       "    border-radius: var(--border-width4);\n",
       "}\n",
       "span.plain {\n",
       "    display: inline-block;\n",
       "    white-space: pre-wrap;\n",
       "}\n",
       ".plain {\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".plain.l,.contnr.l,.contnr.l>.lbl {\n",
       "    border-left-style: dotted\n",
       "}\n",
       ".plain.r,.contnr.r,.contnr.r>.lbl {\n",
       "    border-right-style: dotted\n",
       "}\n",
       ".plain.lno,.contnr.lno,.contnr.lno>.lbl {\n",
       "    border-left-style: none\n",
       "}\n",
       ".plain.rno,.contnr.rno,.contnr.rno>.lbl {\n",
       "    border-right-style: none\n",
       "}\n",
       ".plain.l {\n",
       "    padding-left: 0.2rem;\n",
       "    margin-left: 0.1rem;\n",
       "    border-width: var(--border-width-plain);\n",
       "}\n",
       ".plain.r {\n",
       "    padding-right: 0.2rem;\n",
       "    margin-right: 0.1rem;\n",
       "    border-width: var(--border-width-plain);\n",
       "}\n",
       ".lbl {\n",
       "    font-family: monospace;\n",
       "    margin-top: -1.2rem;\n",
       "    margin-left: 1rem;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding: 0 0.3rem;\n",
       "    border-style: solid;\n",
       "    display: block;\n",
       "    color: var(--label)\n",
       "}\n",
       ".lbl.trm {\n",
       "    background-attachment: local;\n",
       "    margin-top: 0.1rem;\n",
       "    margin-left: 0.1rem;\n",
       "    padding: 0.1rem 0.1rem;\n",
       "    border-style: none;\n",
       "}\n",
       ".lbl.cnul {\n",
       "    font-size: xx-small;\n",
       "}\n",
       ".lbl.c0 {\n",
       "    font-size: small;\n",
       "}\n",
       ".lbl.c1 {\n",
       "    font-size: small;\n",
       "}\n",
       ".lbl.c2 {\n",
       "    font-size: medium;\n",
       "}\n",
       ".lbl.c3 {\n",
       "    font-size: medium;\n",
       "}\n",
       ".lbl.c4 {\n",
       "    font-size: large;\n",
       "}\n",
       ".occs, a:link.occs {\n",
       "    font-size: small;\n",
       "}\n",
       "\n",
       "/* PROVENANCE */\n",
       "\n",
       "div.prov {\n",
       "\tmargin: 2rem;\n",
       "\tpadding: 1rem;\n",
       "\tborder: 0.1rem solid var(--fog-rim);\n",
       "}\n",
       "div.pline {\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "}\n",
       "div.p2line {\n",
       "\tmargin-left: 2em;\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "}\n",
       "div.psline {\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "\tbackground-color: var(--gold-mist-back);\n",
       "}\n",
       "div.pname {\n",
       "\tflex: 0 0 5rem;\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.pval {\n",
       "    flex: 1 1 auto;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--node:               hsla(120, 100%,  20%, 1.0  );\n",
       "\t--label:              hsla(  0, 100%,  20%, 1.0  );\n",
       "\t--section:            hsla(  0, 100%,  25%, 1.0  );\n",
       "\t--structure:          hsla(120, 100%,  25%, 1.0  );\n",
       "\t--features:           hsla(  0,   0%,  30%, 1.0  );\n",
       "  --text-color:         hsla( 60,  80%,  10%, 1.0  );\n",
       "  --lex-color:          hsla(220,  90%,  60%, 1.0  );\n",
       "  --meta-color:         hsla(  0,   0%,  90%, 0.7  );\n",
       "  --meta-width:         0.15rem;\n",
       "  --border-color-nul:   hsla(  0,   0%,  90%, 0.5  );\n",
       "  --border-color0:      hsla(  0,   0%,  90%, 0.9  );\n",
       "  --border-color1:      hsla(  0,   0%,  80%, 0.9  );\n",
       "  --border-color2:      hsla(  0,   0%,  70%, 0.9  );\n",
       "  --border-color3:      hsla(  0,   0%,  80%, 0.8  );\n",
       "  --border-color4:      hsla(  0,   0%,  60%, 0.9  );\n",
       "  --border-width-nul:   0.1rem;\n",
       "  --border-width0:      0.1rem;\n",
       "  --border-width1:      0.15rem;\n",
       "  --border-width2:      0.2rem;\n",
       "  --border-width3:      0.3rem;\n",
       "  --border-width4:      0.25rem;\n",
       "  --border-width-plain: 0.1rem;\n",
       "}\n",
       ".hl {\n",
       "  background-color: var(--hl-strong);\n",
       "}\n",
       "span.hl {\n",
       "\tbackground-color: var(--hl-strong);\n",
       "\tborder-width: 0;\n",
       "\tborder-radius: 0.1rem;\n",
       "\tborder-style: solid;\n",
       "}\n",
       "div.contnr.hl,div.lbl.hl {\n",
       "  background-color: var(--hl-strong);\n",
       "}\n",
       "div.contnr.hl {\n",
       "  border-color: var(--hl-rim) ! important;\n",
       "\tborder-width: 0.2rem ! important;\n",
       "}\n",
       "\n",
       "span.hlbx {\n",
       "\tborder-color: var(--hl-rim);\n",
       "\tborder-width: 0.2rem ! important;\n",
       "\tborder-style: solid;\n",
       "\tborder-radius: 0.3rem;\n",
       "  padding: 0.2rem;\n",
       "  margin: 0.2rem;\n",
       "}\n",
       "\n",
       "span.plain {\n",
       "  display: inline-block;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--hl-strong:        hsla( 60, 100%,  70%, 0.9  );\n",
       "\t--hl-rim:           hsla( 55,  80%,  50%, 1.0  );\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><b>Text-Fabric API:</b> names <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/cheatsheet.html\" title=\"doc\">N F E L T S C TF</a> directly usable</div><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = use(\"tisch:clone\", checkout=\"clone\", hoist=globals())\n",
    "# A = use('tisch:hot', checkout=\"hot\", hoist=globals())\n",
    "# A = use('tisch:latest', checkout=\"latest\", hoist=globals())\n",
    "# A = use('tisch', hoist=globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API\n",
    "\n",
    "At this point it is helpful to throw a quick glance at the text-fabric API documentation\n",
    "(see the links under **API Members** above).\n",
    "\n",
    "The most essential thing for now is that we can use `F` to access the data in the features\n",
    "we've loaded.\n",
    "But there is more, such as `N`, which helps us to walk over the text, as we see in a minute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting\n",
    "\n",
    "In order to get acquainted with the data, we start with the simple task of counting.\n",
    "\n",
    "## Count all nodes\n",
    "We use the\n",
    "[`N.walk()` generator](https://annotation.github.io/text-fabric/tf/core/nodes.html#tf.core.nodes.Nodes.walk)\n",
    "to walk through the nodes.\n",
    "\n",
    "We compared corpus to a gigantic spreadsheet, where the rows correspond to the words.\n",
    "In Text-Fabric, we call the rows `slots`, because they are the textual positions that can be filled with words.\n",
    "\n",
    "We also mentioned that there are also more textual objects.\n",
    "They are the verses, chapters and books.\n",
    "They also correspond to rows in the big spreadsheet.\n",
    "\n",
    "In Text-Fabric we call all these rows *nodes*, and the `N.walk()` generator\n",
    "carries us through those nodes in the textual order.\n",
    "\n",
    "Just one extra thing: the `info` statements generate timed messages.\n",
    "If you use them instead of `print` you'll get a sense of the amount of time that\n",
    "the various processing steps typically need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:13:01.501437Z",
     "start_time": "2018-03-08T10:13:01.452315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Counting nodes ...\n",
      "  0.03s 152077 nodes\n"
     ]
    }
   ],
   "source": [
    "A.indent(reset=True)\n",
    "A.info(\"Counting nodes ...\")\n",
    "\n",
    "i = 0\n",
    "for n in N.walk():\n",
    "    i += 1\n",
    "\n",
    "A.info(\"{} nodes\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are those nodes?\n",
    "Every node has a type, like word, or phrase, sentence.\n",
    "We know that we have approximately 150,000 words and a few other nodes.\n",
    "But what exactly are they?\n",
    "\n",
    "Text-Fabric has two special features, `otype` and `oslots`, that must occur in every Text-Fabric data set.\n",
    "`otype` tells you for each node its type, and you can ask for the number of `slot`s in the text.\n",
    "\n",
    "Here we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:13:05.040545Z",
     "start_time": "2018-03-08T10:13:05.019791Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.otype.slotType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:13:05.855607Z",
     "start_time": "2018-03-08T10:13:05.849069Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137711"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.otype.maxSlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:13:06.387787Z",
     "start_time": "2018-03-08T10:13:06.381228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152077"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.otype.maxNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:13:07.770989Z",
     "start_time": "2018-03-08T10:13:07.764917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('book', 'chapter', 'paragraph', 'lex', 'verse', 'word')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.otype.all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:13:11.588722Z",
     "start_time": "2018-03-08T10:13:11.581305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('book', 5100.407407407408, 137712, 137738),\n",
       " ('chapter', 525.6145038167939, 137739, 138000),\n",
       " ('paragraph', 188.12978142076503, 138001, 138732),\n",
       " ('lex', 25.53040415276233, 146684, 152077),\n",
       " ('verse', 17.319959753490128, 138733, 146683),\n",
       " ('word', 1, 1, 137711))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.levels.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting: above you see all the textual objects, with the average size of their objects,\n",
    "the node where they start, and the node where they end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count individual object types\n",
    "This is an intuitive way to count the number of nodes in each type.\n",
    "Note in passing, how we use the `indent` in conjunction with `info` to produce neat timed\n",
    "and indented progress messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:13:21.547051Z",
     "start_time": "2018-03-08T10:13:21.498807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s counting objects ...\n",
      "   |     0.00s      27 books\n",
      "   |     0.00s     262 chapters\n",
      "   |     0.00s     732 paragraphs\n",
      "   |     0.00s    5394 lexs\n",
      "   |     0.00s    7951 verses\n",
      "   |     0.02s  137711 words\n",
      "  0.02s Done\n"
     ]
    }
   ],
   "source": [
    "A.indent(reset=True)\n",
    "A.info(\"counting objects ...\")\n",
    "\n",
    "for otype in F.otype.all:\n",
    "    i = 0\n",
    "    A.indent(level=1, reset=True)\n",
    "\n",
    "    for n in F.otype.s(otype):\n",
    "        i += 1\n",
    "\n",
    "    A.info(\"{:>7} {}s\".format(i, otype))\n",
    "\n",
    "A.indent(level=0)\n",
    "A.info(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing textual objects\n",
    "\n",
    "We use the A API (the extra power) to peek into the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T09:18:02.282178Z",
     "start_time": "2018-05-18T09:18:02.274117Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"section \">Matthew 3:7</div><div class=\" children\"><div class=\"contnr c0 trm   \" ><div class=\"lbl c0 trm \" ><span class=\"nd\">1000</span> <span class=\"txtu grc\">Σαδδουκαίων </span></div></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"section \">Matthew 17:16</div><div class=\" children\"><div class=\"contnr c0 trm   \" ><div class=\"lbl c0 trm \" ><span class=\"nd\">10000</span> <span class=\"txtu grc\">μαθηταῖς </span></div></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"section \">2_Corinthians 10:3</div><div class=\" children\"><div class=\"contnr c0 trm   \" ><div class=\"lbl c0 trm \" ><span class=\"nd\">100000</span> <span class=\"txtu grc\">οὐ </span></div></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wordShow = (1000, 10000, 100000)\n",
    "for word in wordShow:\n",
    "    A.pretty(word, withNodes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature statistics\n",
    "\n",
    "`F`\n",
    "gives access to all features.\n",
    "Every feature has a method\n",
    "`freqList()`\n",
    "to generate a frequency list of its values, higher frequencies first.\n",
    "Here are the morphological tags (the top 20, at least):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:13:28.926742Z",
     "start_time": "2018-03-08T10:13:28.846500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('CONJ', 16302),\n",
       " ('PREP', 10531),\n",
       " ('ADV', 3763),\n",
       " ('N-NSM', 3471),\n",
       " ('N-GSM', 2932),\n",
       " ('T-NSM', 2902),\n",
       " ('N-ASF', 2876),\n",
       " ('PRT-N', 2670),\n",
       " ('N-ASM', 2456),\n",
       " ('V-PAI-3S', 2271),\n",
       " ('N-GSF', 2187),\n",
       " ('T-GSM', 1904),\n",
       " ('N-NSF', 1601),\n",
       " ('T-ASM', 1574),\n",
       " ('T-ASF', 1526),\n",
       " ('N-DSF', 1460),\n",
       " ('P-GSM', 1358),\n",
       " ('T-GSF', 1292),\n",
       " ('V-2AAI-3S', 1254),\n",
       " ('N-DSM', 1235))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.morph.freqList()[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of verbs, assuming that any word with a morph tag starting with `V-` is a verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28372"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nVerbs = 0\n",
    "\n",
    "for (tag, n) in F.morph.freqList():\n",
    "    if tag.startswith(\"V-\"):\n",
    "        nVerbs += n\n",
    "\n",
    "nVerbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature `anlex_lem` contains lexeme information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('ὁ', 19788),\n",
       " ('καί', 8978),\n",
       " ('αὐτός', 5566),\n",
       " ('σύ', 2903),\n",
       " ('δέ', 2782),\n",
       " ('ἐν', 2744),\n",
       " ('ἐγώ', 2583),\n",
       " ('εἰμί', 2462),\n",
       " ('λέγω', 2259),\n",
       " ('εἰς', 1770),\n",
       " ('οὐ', 1627),\n",
       " ('ὅς', 1405),\n",
       " ('οὗτος', 1381),\n",
       " ('θεός', 1313),\n",
       " ('ὅτι', 1300),\n",
       " ('πᾶς', 1227),\n",
       " ('γάρ', 1037),\n",
       " ('μή', 1034),\n",
       " ('ἐκ', 916),\n",
       " ('Ἰησοῦς', 909))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.anlex_lem.freqList()[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexeme matters\n",
    "\n",
    "## Top 10 frequent verbs\n",
    "\n",
    "If we count the frequency of words, we usually mean the frequency of their\n",
    "corresponding lexemes.\n",
    "\n",
    "There are several methods for working with lexemes.\n",
    "\n",
    "### Method 1: counting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:13:39.960696Z",
     "start_time": "2018-03-08T10:13:39.829670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Collecting data\n",
      "  0.10s Done\n",
      "εἰμί: 2461\n",
      "λέγω: 2258\n",
      "ἔχω: 707\n",
      "ὁράω: 683\n",
      "γίνομαι: 667\n",
      "ἔρχομαι: 627\n",
      "ποιέω: 569\n",
      "ἀκούω: 427\n",
      "δίδωμι: 414\n",
      "οἶδα: 317\n",
      "\n"
     ]
    }
   ],
   "source": [
    "verbs = collections.Counter()\n",
    "A.indent(reset=True)\n",
    "A.info(\"Collecting data\")\n",
    "\n",
    "verbStart = \"V-\"\n",
    "\n",
    "for w in F.otype.s(\"word\"):\n",
    "    if not F.morph.v(w).startswith(verbStart):\n",
    "        continue\n",
    "    verbs[F.anlex_lem.v(w)] += 1\n",
    "\n",
    "A.info(\"Done\")\n",
    "print(\n",
    "    \"\".join(\n",
    "        \"{}: {}\\n\".format(verb, cnt)\n",
    "        for (verb, cnt) in sorted(verbs.items(), key=lambda x: (-x[1], x[0]))[0:10]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexeme distribution\n",
    "\n",
    "Let's do a bit more fancy lexeme stuff.\n",
    "\n",
    "### Hapaxes\n",
    "\n",
    "A hapax can be found by inspecting lexemes and see to how many word nodes they are linked.\n",
    "If that is number is one, we have a hapax.\n",
    "\n",
    "We print 10 hapaxes with their gloss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:13:58.376059Z",
     "start_time": "2018-03-08T10:13:58.247752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.08s 1920 hapaxes found\n",
      "\tΑἰνών\n",
      "\tΑὐγοῦστος\n",
      "\tΒάαλ\n",
      "\tΒαλάκ\n",
      "\tΒαράκ\n",
      "\tΒαραχίας\n",
      "\tΒαριησοῦς\n",
      "\tΒαριωνᾶ\n",
      "\tΒαρτιμαῖος\n",
      "\tΒελιάρ\n"
     ]
    }
   ],
   "source": [
    "A.indent(reset=True)\n",
    "\n",
    "hapax = []\n",
    "lexIndex = collections.defaultdict(list)\n",
    "\n",
    "for n in F.otype.s(\"word\"):\n",
    "    lexIndex[F.anlex_lem.v(n)].append(n)\n",
    "\n",
    "hapax = dict((lex, occs) for (lex, occs) in lexIndex.items() if len(occs) == 1)\n",
    "\n",
    "A.info(\"{} hapaxes found\".format(len(hapax)))\n",
    "\n",
    "for h in sorted(hapax)[0:10]:\n",
    "    print(f\"\\t{h}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want more info on the hapaxes, we get that by means of its *node*.\n",
    "The lexIndex dictionary stores the occurrences of a lexeme as a list of nodes.\n",
    "\n",
    "Let's get the part of speech and the syriac form of those 10 hapaxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:14:00.590376Z",
     "start_time": "2018-03-08T10:14:00.580157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tΑἰνών                N-PRI        Aenon\n",
      "\tΑὐγοῦστος            N-GSM        Augustus\n",
      "\tΒάαλ                 N-PRI        Baal\n",
      "\tΒαλάκ                N-PRI        Balak\n",
      "\tΒαράκ                N-PRI        Barak\n",
      "\tΒαραχίας             N-GSM        Berekiah\n",
      "\tΒαριησοῦς            N-GSM        Bar-Jesus\n",
      "\tΒαριωνᾶ              N-PRI        son of Jonah\n",
      "\tΒαρτιμαῖος           N-NSM        Bartimaeus\n",
      "\tΒελιάρ               N-PRI        needle\n"
     ]
    }
   ],
   "source": [
    "for h in sorted(hapax)[0:10]:\n",
    "    node = hapax[h][0]\n",
    "    print(f\"\\t{F.anlex_lem.v(node):<20} {F.morph.v(node):<12} {F.gloss.v(node)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small occurrence base\n",
    "\n",
    "The occurrence base of a lexeme are the verses, chapters and books in which occurs.\n",
    "Let's look for lexemes that occur in a single chapter.\n",
    "\n",
    "Oh yes, we have already found the hapaxes, we will skip them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:14:03.503420Z",
     "start_time": "2018-03-08T10:14:02.841707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Finding single chapter lexemes\n",
      "  0.56s 2124 single chapter lexemes found\n",
      "Ζάρα [34]\n",
      "Matthew 1:3          Ζάρα   (1x)\n",
      "Θαμάρ [37]\n",
      "Matthew 1:3          Θαμάρ  (1x)\n",
      "Οὐρίας [99]\n",
      "Matthew 1:6          Οὐρίας (1x)\n",
      "Σαλμών [62, 63]\n",
      "Matthew 1:4          Σαλμών (2x)\n",
      "Ἀράμ [47, 48]\n",
      "Matthew 1:3          Ἀράμ   (2x)\n",
      "Ἀσάφ [114, 115]\n",
      "Matthew 1:7          Ἀσάφ   (2x)\n",
      "Ἰωσαφάτ [119, 120]\n",
      "Matthew 1:8          Ἰωσαφάτ (2x)\n",
      "Ῥαχάβ [70]\n",
      "Matthew 1:5          Ῥαχάβ  (1x)\n",
      "Ῥοβοάμ [104, 105]\n",
      "Matthew 1:7          Ῥοβοάμ (2x)\n",
      "Ῥούθ [78]\n",
      "Matthew 1:5          Ῥούθ   (1x)\n"
     ]
    }
   ],
   "source": [
    "A.indent(reset=True)\n",
    "A.info(\"Finding single chapter lexemes\")\n",
    "\n",
    "lexChapterIndex = {}\n",
    "\n",
    "for (lex, occs) in lexIndex.items():\n",
    "    lexChapterIndex[lex] = set(L.u(n, otype=\"chapter\")[0] for n in occs)\n",
    "\n",
    "singleCh = [\n",
    "    (lex, occs)\n",
    "    for (lex, occs) in lexIndex.items()\n",
    "    if len(lexChapterIndex.get(lex, [])) == 1\n",
    "]\n",
    "\n",
    "A.info(\"{} single chapter lexemes found\".format(len(singleCh)))\n",
    "\n",
    "for (lex, occs) in sorted(singleCh[0:10]):\n",
    "    print(lex, occs)\n",
    "    print(\n",
    "        \"{:<20} {:<6} ({}x)\".format(\n",
    "            \"{} {}:{}\".format(*T.sectionFromNode(occs[0])),\n",
    "            lex,\n",
    "            len(occs),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confined to books\n",
    "\n",
    "As a final exercise with lexemes, lets make a list of all books, and show their total number of lexemes and\n",
    "the number of lexemes that occur exclusively in that book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:14:07.670486Z",
     "start_time": "2018-03-08T10:14:07.479785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Making book-lexeme index\n",
      "  0.12s Found 5394 lexemes\n"
     ]
    }
   ],
   "source": [
    "A.indent(reset=True)\n",
    "A.info(\"Making book-lexeme index\")\n",
    "\n",
    "allBook = collections.defaultdict(set)\n",
    "allLex = set()\n",
    "\n",
    "for b in F.otype.s(\"book\"):\n",
    "    for w in L.d(b, \"word\"):\n",
    "        ln = F.anlex_lem.v(w)\n",
    "        allBook[b].add(ln)\n",
    "        allLex.add(ln)\n",
    "\n",
    "A.info(\"Found {} lexemes\".format(len(allLex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:14:09.712557Z",
     "start_time": "2018-03-08T10:14:09.068800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Finding single book lexemes\n",
      "  0.56s found 2404 single book lexemes\n"
     ]
    }
   ],
   "source": [
    "A.indent(reset=True)\n",
    "A.info(\"Finding single book lexemes\")\n",
    "\n",
    "lexBookIndex = {}\n",
    "\n",
    "for (lex, occs) in lexIndex.items():\n",
    "    lexBookIndex[lex] = set(L.u(n, otype=\"book\")[0] for n in occs)\n",
    "\n",
    "singleBookLex = collections.defaultdict(set)\n",
    "for (lex, books) in lexBookIndex.items():\n",
    "    if len(books) == 1:\n",
    "        singleBookLex[list(books)[0]].add(lex)\n",
    "\n",
    "singleBook = {book: len(lexs) for (book, lexs) in singleBookLex.items()}\n",
    "\n",
    "A.info(\"found {} single book lexemes\".format(sum(singleBook.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:14:10.997607Z",
     "start_time": "2018-03-08T10:14:10.964980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book                 #all #own %own\n",
      "-----------------------------------\n",
      "Acts                 2017  555 27.5%\n",
      "Luke                 2039  339 16.6%\n",
      "Hebrews              1026  155 15.1%\n",
      "2_Peter               398   57 14.3%\n",
      "Revelation            911  126 13.8%\n",
      "1_Timothy             537   74 13.8%\n",
      "2_Timothy             453   62 13.7%\n",
      "Romans               1058  132 12.5%\n",
      "2_Corinthians         786   94 12.0%\n",
      "James                 554   63 11.4%\n",
      "1_Peter               542   60 11.1%\n",
      "1_Corinthians         954  103 10.8%\n",
      "Titus                 298   31 10.4%\n",
      "John                 1018  104 10.2%\n",
      "Philippians           443   42  9.5%\n",
      "Matthew              1664  149  9.0%\n",
      "Colossians            430   38  8.8%\n",
      "Ephesians             527   41  7.8%\n",
      "Jude                  226   15  6.6%\n",
      "Galatians             519   34  6.6%\n",
      "Mark                 1330   84  6.3%\n",
      "1_Thessalonians       362   19  5.2%\n",
      "Philemon              141    7  5.0%\n",
      "2_Thessalonians       251   10  4.0%\n",
      "3_John                108    4  3.7%\n",
      "2_John                 96    2  2.1%\n",
      "1_John                233    4  1.7%\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"{:<20}{:>5}{:>5}{:>5}\\n{}\".format(\n",
    "        \"book\",\n",
    "        \"#all\",\n",
    "        \"#own\",\n",
    "        \"%own\",\n",
    "        \"-\" * 35,\n",
    "    )\n",
    ")\n",
    "booklist = []\n",
    "\n",
    "for b in F.otype.s(\"book\"):\n",
    "    book = T.bookName(b)\n",
    "    a = len(allBook[b])\n",
    "    o = singleBook.get(b, 0)\n",
    "    p = 100 * o / a\n",
    "    booklist.append((book, a, o, p))\n",
    "\n",
    "for x in sorted(booklist, key=lambda e: (-e[3], -e[1], e[0])):\n",
    "    print(\"{:<20} {:>4} {:>4} {:>4.1f}%\".format(*x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer API\n",
    "We travel upwards and downwards, forwards and backwards through the nodes.\n",
    "The Layer-API (`L`) provides functions: `u()` for going up, and `d()` for going down,\n",
    "`n()` for going to next nodes and `p()` for going to previous nodes.\n",
    "\n",
    "These directions are indirect notions: nodes are just numbers, but by means of the\n",
    "`oslots` feature they are linked to slots. One node *contains* an other node, if the one is linked to a set of slots that contains the set of slots that the other is linked to.\n",
    "And one if next or previous to an other, if its slots follow of precede the slots of the other one.\n",
    "\n",
    "`L.u(node)` **Up** is going to nodes that embed `node`.\n",
    "\n",
    "`L.d(node)` **Down** is the opposite direction, to those that are contained in `node`.\n",
    "\n",
    "`L.n(node)` **Next** are the next *adjacent* nodes, i.e. nodes whose first slot comes immediately after the last slot of `node`.\n",
    "\n",
    "`L.p(node)` **Previous** are the previous *adjacent* nodes, i.e. nodes whose last slot comes immediately before the first slot of `node`.\n",
    "\n",
    "All these functions yield nodes of all possible otypes.\n",
    "By passing an optional parameter, you can restrict the results to nodes of that type.\n",
    "\n",
    "The result are ordered according to the order of things in the text.\n",
    "\n",
    "The functions return always a tuple, even if there is just one node in the result.\n",
    "\n",
    "## Going up\n",
    "We go from the first word to the book it contains.\n",
    "Note the `[0]` at the end. You expect one book, yet `L` returns a tuple.\n",
    "To get the only element of that tuple, you need to do that `[0]`.\n",
    "\n",
    "If you are like me, you keep forgetting it, and that will lead to weird error messages later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:14:15.350367Z",
     "start_time": "2018-03-08T10:14:15.343039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137712\n"
     ]
    }
   ],
   "source": [
    "firstBook = L.u(1, otype=\"book\")[0]\n",
    "print(firstBook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see all the containing objects of word 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:14:17.364197Z",
     "start_time": "2018-03-08T10:14:17.352186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 3 is contained in book 137712\n",
      "word 3 is contained in chapter 137739\n",
      "word 3 is contained in paragraph 138001\n",
      "word 3 is contained in lex 146686\n",
      "word 3 is contained in verse 138733\n"
     ]
    }
   ],
   "source": [
    "w = 3\n",
    "for otype in F.otype.all:\n",
    "    if otype == F.otype.slotType:\n",
    "        continue\n",
    "    up = L.u(w, otype=otype)\n",
    "    upNode = \"x\" if len(up) == 0 else up[0]\n",
    "    print(\"word {} is contained in {} {}\".format(w, otype, upNode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going next\n",
    "Let's go to the next nodes of the first book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:14:20.775341Z",
     "start_time": "2018-03-08T10:14:20.762875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  18259: word          first slot=18259 , last slot=18259 \n",
      " 139800: verse         first slot=18259 , last slot=18263 \n",
      " 138090: paragraph     first slot=18259 , last slot=18439 \n",
      " 137767: chapter       first slot=18259 , last slot=18954 \n",
      " 137713: book          first slot=18259 , last slot=29495 \n"
     ]
    }
   ],
   "source": [
    "afterFirstBook = L.n(firstBook)\n",
    "for n in afterFirstBook:\n",
    "    print(\n",
    "        \"{:>7}: {:<13} first slot={:<6}, last slot={:<6}\".format(\n",
    "            n,\n",
    "            F.otype.v(n),\n",
    "            E.oslots.s(n)[0],\n",
    "            E.oslots.s(n)[-1],\n",
    "        )\n",
    "    )\n",
    "secondBook = L.n(firstBook, otype=\"book\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going previous\n",
    "\n",
    "And let's see what is right before the second book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:14:23.300424Z",
     "start_time": "2018-03-08T10:14:23.292226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 137712: book          first slot=1     , last slot=18258 \n",
      " 137766: chapter       first slot=17933 , last slot=18258 \n",
      " 138089: paragraph     first slot=18180 , last slot=18258 \n",
      " 139799: verse         first slot=18238 , last slot=18258 \n",
      "  18258: word          first slot=18258 , last slot=18258 \n"
     ]
    }
   ],
   "source": [
    "for n in L.p(secondBook):\n",
    "    print(\n",
    "        \"{:>7}: {:<13} first slot={:<6}, last slot={:<6}\".format(\n",
    "            n,\n",
    "            F.otype.v(n),\n",
    "            E.oslots.s(n)[0],\n",
    "            E.oslots.s(n)[-1],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We go to the chapters of the second book, and just count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:14:25.969860Z",
     "start_time": "2018-03-08T10:14:25.957084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "chapters = L.d(secondBook, otype=\"chapter\")\n",
    "print(len(chapters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first verse\n",
    "We pick the first verse and the first word, and explore what is above and below them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:14:28.468853Z",
     "start_time": "2018-03-08T10:14:28.416537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 1\n",
      "   |   UP\n",
      "   |      |   146684          lex\n",
      "   |      |   138733          verse\n",
      "   |      |   138001          paragraph\n",
      "   |      |   137739          chapter\n",
      "   |      |   137712          book\n",
      "   |   DOWN\n",
      "   |      |   \n",
      "Node 138733\n",
      "   |   UP\n",
      "   |      |   138001          paragraph\n",
      "   |      |   137739          chapter\n",
      "   |      |   137712          book\n",
      "   |   DOWN\n",
      "   |      |   1               word\n",
      "   |      |   2               word\n",
      "   |      |   3               word\n",
      "   |      |   4               word\n",
      "   |      |   5               word\n",
      "   |      |   6               word\n",
      "   |      |   7               word\n",
      "   |      |   8               word\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for n in [1, L.u(1, otype=\"verse\")[0]]:\n",
    "    A.indent(level=0)\n",
    "    A.info(\"Node {}\".format(n), tm=False)\n",
    "    A.indent(level=1)\n",
    "    A.info(\"UP\", tm=False)\n",
    "    A.indent(level=2)\n",
    "    A.info(\"\\n\".join([\"{:<15} {}\".format(u, F.otype.v(u)) for u in L.u(n)]), tm=False)\n",
    "    A.indent(level=1)\n",
    "    A.info(\"DOWN\", tm=False)\n",
    "    A.indent(level=2)\n",
    "    A.info(\"\\n\".join([\"{:<15} {}\".format(u, F.otype.v(u)) for u in L.d(n)]), tm=False)\n",
    "A.indent(level=0)\n",
    "A.info(\"Done\", tm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text API\n",
    "\n",
    "So far, we have mainly seen nodes and their numbers, and the names of node types.\n",
    "You would almost forget that we are dealing with text.\n",
    "So let's try to see some text.\n",
    "\n",
    "In the same way as `F` gives access to feature data,\n",
    "`T` gives access to the text.\n",
    "That is also feature data, but you can tell Text-Fabric which features are specifically\n",
    "carrying the text, and in return Text-Fabric offers you\n",
    "a Text API: `T`.\n",
    "\n",
    "## Formats\n",
    "Syriac text can be represented in a number of ways:\n",
    "\n",
    "* in transliteration, or in Syriac characters,\n",
    "* showing the actual text or only the lexemes,\n",
    "\n",
    "If you wonder where the information about text formats is stored:\n",
    "not in the program text-fabric, but in the data set.\n",
    "It has a feature `otext`, which specifies the formats and which features\n",
    "must be used to produce them. `otext` is the third special feature in a TF data set,\n",
    "next to `otype` and `oslots`.\n",
    "It is an optional feature.\n",
    "If it is absent, there will be no `T` API.\n",
    "\n",
    "Here is a list of all available formats in this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:14:31.965590Z",
     "start_time": "2018-03-08T10:14:31.955945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lex-an-full', 'lex-str-full', 'text-orig-full', 'text-orig-ketiv']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(T.formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the formats\n",
    "\n",
    "We can pretty display in other formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"section \">Matthew 3:7</div><div class=\" children\"><div class=\"contnr c0 trm   \" ><div class=\"lbl c0 trm \" ><span class=\"txtu grc\">Σαδδουκαίων </span></div></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"section \">Matthew 17:16</div><div class=\" children\"><div class=\"contnr c0 trm   \" ><div class=\"lbl c0 trm \" ><span class=\"txtu grc\">μαθηταῖς </span></div></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"section \">2_Corinthians 10:3</div><div class=\" children\"><div class=\"contnr c0 trm   \" ><div class=\"lbl c0 trm \" ><span class=\"txtu grc\">οὐ </span></div></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for word in wordShow:\n",
    "    A.pretty(word, fmt=\"text-orig-ketiv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use those formats to print out the first verse of the Hebrew Bible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:14:35.240131Z",
     "start_time": "2018-03-08T10:14:35.231752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lex-an-full:\n",
      "\tβίβλος γένεσις Ἰησοῦς Χριστός υἱός Δαυίδ υἱός Ἀβραάμ Ἀβραάμ γεννάω ὁ \n",
      "lex-str-full:\n",
      "\tβίβλος γένεσις Ἰησοῦς Χριστός υἱός Δαβίδ υἱός Ἀβραάμ Ἀβραάμ γεννάω ὁ \n",
      "text-orig-full:\n",
      "\tΒίβλος γενέσεως Ἰησοῦ Χριστοῦ υἱοῦ Δαυεὶδ υἱοῦ Ἀβραάμ. Ἀβραὰμ ἐγέννησεν τὸν \n",
      "text-orig-ketiv:\n",
      "\tΒίβλος γενέσεως Ἰησοῦ Χριστοῦ υἱοῦ Δαυεὶδ υἱοῦ Ἀβραάμ. Ἀβραὰμ ἐγέννησεν τὸν \n"
     ]
    }
   ],
   "source": [
    "for fmt in sorted(T.formats):\n",
    "    print(\"{}:\\n\\t{}\".format(fmt, T.text(range(1, 12), fmt=fmt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do not specify a format, the **default** format is used (`text-orig-full`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:14:37.681491Z",
     "start_time": "2018-03-08T10:14:37.674538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Βίβλος γενέσεως Ἰησοῦ Χριστοῦ υἱοῦ Δαυεὶδ υἱοῦ Ἀβραάμ. Ἀβραὰμ ἐγέννησεν τὸν \n"
     ]
    }
   ],
   "source": [
    "print(T.text(range(1, 12)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole text in all formats in less than a second\n",
    "Part of the pleasure of working with computers is that they can crunch massive amounts of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:14:40.870244Z",
     "start_time": "2018-03-08T10:14:39.998071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s writing plain text of whole New Testament in all formats\n",
      "  1.05s done 4 formats\n",
      "lex-an-full\n",
      "βίβλος γένεσις Ἰησοῦς Χριστός υἱός Δαυίδ υἱός Ἀβραάμ \n",
      "Ἀβραάμ γεννάω ὁ Ἰσαάκ Ἰσαάκ δέ γεννάω ὁ Ἰακώβ Ἰακώβ δέ γεννάω ὁ Ἰούδας καί ὁ ἀδελφός αὐτός \n",
      "Ἰούδας δέ γεννάω ὁ Φάρες καί ὁ Ζάρα ἐκ ὁ Θαμάρ Φάρες δέ γεννάω ὁ Ἑσρώμ Ἑσρώμ δέ γεννάω ὁ Ἀράμ \n",
      "Ἀράμ δέ γεννάω ὁ Ἀμιναδάβ Ἀμιναδάβ δέ γεννάω ὁ Ναασσών Ναασσών δέ γεννάω ὁ Σαλμών \n",
      "Σαλμών δέ γεννάω ὁ Βόες ἐκ ὁ Ῥαχάβ Βόες δέ γεννάω ὁ Ἰωβήδ ἐκ ὁ Ῥούθ Ἰωβήδ δέ γεννάω ὁ Ἰεσσαί \n",
      "\n",
      "lex-str-full\n",
      "βίβλος γένεσις Ἰησοῦς Χριστός υἱός Δαβίδ υἱός Ἀβραάμ \n",
      "Ἀβραάμ γεννάω ὁ Ἰσαάκ Ἰσαάκ δέ γεννάω ὁ Ἰακώβ Ἰακώβ δέ γεννάω ὁ Ἰούδας καί ὁ ἀδελφός αὐτός \n",
      "Ἰούδας δέ γεννάω ὁ Φάρες καί ὁ Ζαρά ἐκ ὁ Θάμαρ Φάρες δέ γεννάω ὁ Ἐσρώμ Ἐσρώμ δέ γεννάω ὁ Ἀράμ \n",
      "Ἀράμ δέ γεννάω ὁ Ἀμιναδάβ Ἀμιναδάβ δέ γεννάω ὁ Ναασσών Ναασσών δέ γεννάω ὁ Σαλμών \n",
      "Σαλμών δέ γεννάω ὁ Βοόζ ἐκ ὁ Ῥαχάβ Βοόζ δέ γεννάω ὁ Ὠβήδ ἐκ ὁ Ῥούθ Ὠβήδ δέ γεννάω ὁ Ἰεσσαί \n",
      "\n",
      "text-orig-full\n",
      "Βίβλος γενέσεως Ἰησοῦ Χριστοῦ υἱοῦ Δαυεὶδ υἱοῦ Ἀβραάμ. \n",
      "Ἀβραὰμ ἐγέννησεν τὸν Ἰσαάκ, Ἰσαὰκ δὲ ἐγέννησεν τὸν Ἰακώβ, Ἰακὼβ δὲ ἐγέννησεν τὸν Ἰούδαν καὶ τοὺς ἀδελφοὺς αὐτοῦ, \n",
      "Ἰούδας δὲ ἐγέννησεν τὸν Φάρες καὶ τὸν Ζάρα ἐκ τῆς Θαμάρ, Φάρες δὲ ἐγέννησεν τὸν Ἑσρώμ, Ἑσρὼμ δὲ ἐγέννησεν τὸν Ἀράμ, \n",
      "Ἀρὰμ δὲ ἐγέννησεν τὸν Ἀμιναδάβ, Ἀμιναδὰβ δὲ ἐγέννησεν τὸν Ναασσών, Ναασσὼν δὲ ἐγέννησεν τὸν Σαλμών, \n",
      "Σαλμὼν δὲ ἐγέννησεν τὸν Βόες ἐκ τῆς Ῥαχάβ, Βόες δὲ ἐγέννησεν τὸν Ἰωβὴδ ἐκ τῆς Ῥούθ, Ἰωβὴδ δὲ ἐγέννησεν τὸν Ἰεσσαί, \n",
      "\n",
      "text-orig-ketiv\n",
      "Βίβλος γενέσεως Ἰησοῦ Χριστοῦ υἱοῦ Δαυεὶδ υἱοῦ Ἀβραάμ. \n",
      "Ἀβραὰμ ἐγέννησεν τὸν Ἰσαάκ, Ἰσαὰκ δὲ ἐγέννησεν τὸν Ἰακώβ, Ἰακὼβ δὲ ἐγέννησεν τὸν Ἰούδαν καὶ τοὺς ἀδελφοὺς αὐτοῦ, \n",
      "Ἰούδας δὲ ἐγέννησεν τὸν Φάρες καὶ τὸν Ζάρα ἐκ τῆς Θαμάρ, Φάρες δὲ ἐγέννησεν τὸν Ἑσρώμ, Ἑσρὼμ δὲ ἐγέννησεν τὸν Ἀράμ, \n",
      "Ἀρὰμ δὲ ἐγέννησεν τὸν Ἀμιναδάβ, Ἀμιναδὰβ δὲ ἐγέννησεν τὸν Ναασσών, Ναασσὼν δὲ ἐγέννησεν τὸν Σαλμών, \n",
      "Σαλμὼν δὲ ἐγέννησεν τὸν Βόες ἐκ τῆς Ῥαχάβ, Βόες δὲ ἐγέννησεν τὸν Ἰωβὴδ ἐκ τῆς Ῥούθ, Ἰωβὴδ δὲ ἐγέννησεν τὸν Ἰεσσαί, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "A.indent(reset=True)\n",
    "A.info(\"writing plain text of whole New Testament in all formats\")\n",
    "\n",
    "text = collections.defaultdict(list)\n",
    "\n",
    "for v in F.otype.s(\"verse\"):\n",
    "    words = L.d(v, \"word\")\n",
    "    for fmt in sorted(T.formats):\n",
    "        text[fmt].append(T.text(words, fmt=fmt))\n",
    "\n",
    "A.info(\"done {} formats\".format(len(text)))\n",
    "\n",
    "for fmt in sorted(text):\n",
    "    print(\"{}\\n{}\\n\".format(fmt, \"\\n\".join(text[fmt][0:5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The full plain text\n",
    "We write a few formats to file, in your `Downloads` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:14:43.743463Z",
     "start_time": "2018-03-08T10:14:43.726787Z"
    }
   },
   "outputs": [],
   "source": [
    "orig = \"text-orig-full\"\n",
    "trans = \"text-trans-full\"\n",
    "for fmt in (orig, trans):\n",
    "    with open(os.path.expanduser(f\"~/Downloads/Tischendorf-{fmt}.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(text[fmt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:14:44.766248Z",
     "start_time": "2018-03-08T10:14:44.618476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Βίβλος γενέσεως Ἰησοῦ Χριστοῦ υἱοῦ Δαυεὶδ υἱοῦ Ἀβραάμ. \n",
      "Ἀβραὰμ ἐγέννησεν τὸν Ἰσαάκ, Ἰσαὰκ δὲ ἐγέννησεν τὸν Ἰακώβ, Ἰακὼβ δὲ ἐγέννησεν τὸν Ἰούδαν καὶ τοὺς ἀδελφοὺς αὐτοῦ, \n",
      "Ἰούδας δὲ ἐγέννησεν τὸν Φάρες καὶ τὸν Ζάρα ἐκ τῆς Θαμάρ, Φάρες δὲ ἐγέννησεν τὸν Ἑσρώμ, Ἑσρὼμ δὲ ἐγέννησεν τὸν Ἀράμ, \n",
      "Ἀρὰμ δὲ ἐγέννησεν τὸν Ἀμιναδάβ, Ἀμιναδὰβ δὲ ἐγέννησεν τὸν Ναασσών, Ναασσὼν δὲ ἐγέννησεν τὸν Σαλμών, \n",
      "Σαλμὼν δὲ ἐγέννησεν τὸν Βόες ἐκ τῆς Ῥαχάβ, Βόες δὲ ἐγέννησεν τὸν Ἰωβὴδ ἐκ τῆς Ῥούθ, Ἰωβὴδ δὲ ἐγέννησεν τὸν Ἰεσσαί, \n",
      "Ἰεσσαὶ δὲ ἐγέννησεν τὸν Δαυεὶδ τὸν βασιλέα. Δαυεὶδ δὲ ἐγέννησεν τὸν Σολομῶνα ἐκ τῆς τοῦ Οὐρίου, \n",
      "Σολομὼν δὲ ἐγέννησεν τὸν Ῥοβοάμ, Ῥοβοὰμ δὲ ἐγέννησεν τὸν Ἀβιά, Ἀβιὰ δὲ ἐγέννησεν τὸν Ἀσάφ, \n",
      "Ἀσὰφ δὲ ἐγέννησεν τὸν Ἰωσαφάτ, Ἰωσαφὰτ δὲ ἐγέννησεν τὸν Ἰωράμ, Ἰωρὰμ δὲ ἐγέννησεν τὸν Ὀζείαν, \n",
      "Ὀζείας δὲ ἐγέννησεν τὸν Ἰωαθάμ, Ἰωαθὰμ δὲ ἐγέννησεν τὸν Ἀχάζ, Ἀχὰζ δὲ ἐγέννησεν τὸν Ἑζεκίαν, \n",
      "Ἑζεκίας δὲ ἐγέννησεν τὸν Μανασσῆ, Μανασσῆς δὲ ἐγέννησεν τὸν Ἀμώς, Ἀμὼς δὲ ἐγέννησεν τὸν Ἰωσείαν, \n",
      "Ἰωσείας δὲ ἐγέννησεν τὸν Ἰεχονίαν καὶ τοὺς ἀδελφοὺς αὐτοῦ ἐπὶ τῆς μετοικεσίας Βαβυλῶνος. \n",
      "μετὰ δὲ τὴν μετοικεσίαν Βαβυλῶνος Ἰεχονίας ἐγέννησεν τὸν Σαλαθιήλ, Σαλαθιὴλ δὲ ἐγέννησεν τὸν Ζοροβάβελ, \n",
      "Ζοροβάβελ δὲ ἐγέννησεν τὸν Ἀβιούδ, Ἀβιοὺδ δὲ ἐγέννησεν τὸν Ἐλιακείμ, Ἐλιακεὶμ δὲ ἐγέννησεν τὸν Ἀζώρ, \n",
      "Ἀζὼρ δὲ ἐγέννησεν τὸν Σαδώκ, Σαδὼκ δὲ ἐγέννησεν τὸν Ἀχείμ, Ἀχεὶμ δὲ ἐγέννησεν τὸν Ἐλιούδ, \n",
      "Ἐλιοὺδ δὲ ἐγέννησεν τὸν Ἐλεάζαρ, Ἐλεάζαρ δὲ ἐγέννησεν τὸν Μαθθάν, Μαθθὰν δὲ ἐγέννησεν τὸν Ἰακώβ, \n",
      "Ἰακὼβ δὲ ἐγέννησεν τὸν Ἰωσὴφ τὸν ἄνδρα Μαρίας, ἐξ ἧς ἐγεννήθη Ἰησοῦς ὁ λεγόμενος Χριστός. \n",
      "Πᾶσαι οὖν αἱ γενεαὶ ἀπὸ Ἀβραὰμ ἕως Δαυεὶδ γενεαὶ δεκατέσσαρες, καὶ ἀπὸ Δαυεὶδ ἕως τῆς μετοικεσίας Βαβυλῶνος γενεαὶ δεκατέσσαρες, καὶ ἀπὸ τῆς μετοικεσίας Βαβυλῶνος ἕως τοῦ Χριστοῦ γενεαὶ δεκατέσσαρες. \n",
      "Τοῦ δὲ Ἰησοῦ Χριστοῦ ἡ γένεσις οὕτως ἦν. μνηστευθείσης τῆς μητρὸς αὐτοῦ Μαρίας τῷ Ἰωσήφ, πρὶν ἢ συνελθεῖν αὐτοὺς εὑρέθη ἐν γαστρὶ ἔχουσα ἐκ πνεύματος ἁγίου. \n",
      "Ἰωσὴφ δὲ ὁ ἀνὴρ αὐτῆς, δίκαιος ὢν καὶ μὴ θέλων αὐτὴν δειγματίσαι, ἐβουλήθη λάθρᾳ ἀπολῦσαι αὐτήν. \n",
      "ταῦτα δὲ αὐτοῦ ἐνθυμηθέντος ἰδοὺ ἄγγελος κυρίου κατ’ ὄναρ ἐφάνη αὐτῷ λέγων, Ἰωσὴφ υἱὸς Δαυείδ, μὴ φοβηθῇς παραλαβεῖν Μαρίαμ τὴν γυναῖκά σου, τὸ γὰρ ἐν αὐτῇ γεννηθὲν ἐκ πνεύματός ἐστιν ἁγίου· \n"
     ]
    }
   ],
   "source": [
    "!head -n 20 ~/Downloads/Tischendorf-{orig}.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book names\n",
    "\n",
    "For Bible book names, we can use several languages.\n",
    "Well, in this case we have just English.\n",
    "\n",
    "### Languages\n",
    "Here are the languages that we can use for book names.\n",
    "These languages come from the features `book@ll`, where `ll` is a two letter\n",
    "ISO language code. Have a look in your data directory, you can't miss them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:14:47.152762Z",
     "start_time": "2018-03-08T10:14:47.145420Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': {'language': 'default', 'languageEnglish': 'default'}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections\n",
    "\n",
    "A section is a book, a chapter or a verse.\n",
    "Knowledge of sections is not baked into Text-Fabric.\n",
    "The config feature `otext.tf` may specify three section levels, and tell\n",
    "what the corresponding node types and features are.\n",
    "\n",
    "From that knowledge it can construct mappings from nodes to sections, e.g. from verse\n",
    "nodes to tuples of the form:\n",
    "\n",
    "    (bookName, chapterNumber, verseNumber)\n",
    "\n",
    "Here are examples of getting the section that corresponds to a node and vice versa.\n",
    "\n",
    "**NB:** `sectionFromNode` always delivers a verse specification, either from the\n",
    "first slot belonging to that node, or, if `lastSlot`, from the last slot\n",
    "belonging to that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:14:49.175819Z",
     "start_time": "2018-03-08T10:14:49.151007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "section of first word          ('Matthew', 1, 1)\n",
      "node of Matthew 1:1            138733\n",
      "node of book Matthew           137712\n",
      "node of chapter Matthew 1      137739\n",
      "section of book node           ('1_Thessalonians', 2, 11)\n",
      "idem, now last word            ('1_Thessalonians', 2, 11)\n",
      "section of chapter node        ('1_Thessalonians', 2, 13)\n",
      "idem, now last word            ('1_Thessalonians', 2, 13)\n"
     ]
    }
   ],
   "source": [
    "for x in (\n",
    "    (\"section of first word\", T.sectionFromNode(1)),\n",
    "    (\"node of Matthew 1:1\", T.nodeFromSection((\"Matthew\", 1, 1))),\n",
    "    (\"node of book Matthew\", T.nodeFromSection((\"Matthew\",))),\n",
    "    (\"node of chapter Matthew 1\", T.nodeFromSection((\"Matthew\", 1))),\n",
    "    (\"section of book node\", T.sectionFromNode(109641)),\n",
    "    (\"idem, now last word\", T.sectionFromNode(109641, lastSlot=True)),\n",
    "    (\"section of chapter node\", T.sectionFromNode(109668)),\n",
    "    (\"idem, now last word\", T.sectionFromNode(109668, lastSlot=True)),\n",
    "):\n",
    "    print(\"{:<30} {}\".format(*x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "By now you have an impression how to compute around in the text.\n",
    "While this is still the beginning, I hope you already sense the power of unlimited programmatic access\n",
    "to all the bits and bytes in the data set.\n",
    "\n",
    "Here are a few directions for unleashing that power.\n",
    "\n",
    "## Search\n",
    "Text-Fabric contains a flexible search engine, that does not only work for this data,\n",
    "but also for data that you add to it.\n",
    "There is a tutorial dedicated to [search](search.ipynb).\n",
    "\n",
    "## Add your own data\n",
    "If you study the additional data, you can observe how that data is created and also\n",
    "how it is turned into a text-fabric data module.\n",
    "The last step is incredibly easy. You can write out every Python dictionary where the keys are numbers\n",
    "and the values string or numbers as a Text-Fabric feature.\n",
    "When you are creating data, you have already constructed those dictionaries, so writing\n",
    "them out is just one method call.\n",
    "\n",
    "You can then easily share your new features on GitHub, so that your colleagues everywhere\n",
    "can try it out for themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to Emdros MQL\n",
    "\n",
    "[EMDROS](http://emdros.org), written by Ulrik Petersen,\n",
    "is a text database system with the powerful *topographic* query language MQL.\n",
    "The ideas are based on a model devised by Christ-Jan Doedens in\n",
    "[Text Databases: One Database Model and Several Retrieval Languages](https://books.google.nl/books?id=9ggOBRz1dO4C).\n",
    "\n",
    "Text-Fabric's model of slots, nodes and edges is a fairly straightforward translation of the models of Christ-Jan Doedens and Ulrik Petersen.\n",
    "\n",
    "[SHEBANQ](https://shebanq.ancient-data.org) uses EMDROS to offer users to execute and save MQL queries against the Hebrew Text Database of the ETCBC.\n",
    "\n",
    "So it is kind of logical and convenient to be able to work with a Text-Fabric resource through MQL.\n",
    "\n",
    "If you have obtained an MQL dataset somehow, you can turn it into a text-fabric data set by `importMQL()`,\n",
    "which we will not show here.\n",
    "\n",
    "And if you want to export a Text-Fabric data set to MQL, that is also possible.\n",
    "\n",
    "After the `Fabric(modules=...)` call, you can call `exportMQL()` in order to save all features of the\n",
    "indicated modules into a big MQL dump, which can be imported by an EMDROS database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T10:15:03.472718Z",
     "start_time": "2018-03-08T10:14:55.847533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Checking features of dataset mytisch\n",
      "  0.00s 14 features to export to MQL ...\n",
      "  0.00s Loading 14 features\n",
      "  0.01s Writing enumerations\n",
      "\tbook           :   27 values, 11 not a name, e.g. «1_Corinthians»\n",
      "\tbook_code      :   27 values, 11 not a name, e.g. «1CO»\n",
      "\tvrsnum         :   58 values, 58 not a name, e.g. «1»\n",
      "  0.08s Mapping 14 features onto 6 object types\n",
      "  0.19s Writing 14 features as data in 6 object types\n",
      "   |     0.00s word data ...\n",
      "   |      |     0.50s batch of size               10.1MB with   50000 of   50000 words\n",
      "   |      |     0.97s batch of size               10.1MB with   50000 of  100000 words\n",
      "   |      |     1.32s batch of size                7.7MB with   37711 of  137711 words\n",
      "   |     1.32s word data: 137711 objects\n",
      "   |     0.00s verse data ...\n",
      "   |      |     0.04s batch of size              598.5KB with    7951 of    7951 verses\n",
      "   |     0.04s verse data: 7951 objects\n",
      "   |     0.00s lex data ...\n",
      "   |      |     0.10s batch of size                1.4MB with    5394 of    5394 lexs\n",
      "   |     0.10s lex data: 5394 objects\n",
      "   |     0.00s paragraph data ...\n",
      "   |      |     0.01s batch of size               54.5KB with     732 of     732 paragraphs\n",
      "   |     0.01s paragraph data: 732 objects\n",
      "   |     0.00s chapter data ...\n",
      "   |      |     0.01s batch of size               20.2KB with     262 of     262 chapters\n",
      "   |     0.01s chapter data: 262 objects\n",
      "   |     0.00s book data ...\n",
      "   |      |     0.01s batch of size                2.7KB with      27 of      27 books\n",
      "   |     0.01s book data: 27 objects\n",
      "  1.69s Done\n"
     ]
    }
   ],
   "source": [
    "TF.exportMQL(\"mytisch\", \"~/Downloads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a file `~/Downloads/mysyrnt.mql` of 52 MB.\n",
    "You can import it into an Emdros database by saying:\n",
    "\n",
    "    cd ~/Downloads\n",
    "    rm mysyrnt\n",
    "    mql -b 3 < mysyrnt.mql\n",
    "\n",
    "The result is an SQLite3 database `mysyrnt` in the same directory (17 MB).\n",
    "You can run a query against it by creating a text file test.mql with this contents:\n",
    "\n",
    "    select all objects where\n",
    "    [verse\n",
    "        [word FOCUS lexeme_ascii = 'WME']\n",
    "    ]\n",
    "\n",
    "And then say\n",
    "\n",
    "    mql -b 3 -d mysyrnt test.mql\n",
    "\n",
    "You will see raw query results: all word occurrences that belong to lexemes with `make` in their gloss.\n",
    "\n",
    "It is not very pretty, and probably you should use a more visual Emdros tool to run those queries.\n",
    "You see a lot of node numbers, but the good thing is, you can look those node numbers up in Text-Fabric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "CC-BY Dirk Roorda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
